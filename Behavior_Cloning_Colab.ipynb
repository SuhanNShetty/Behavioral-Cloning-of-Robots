{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Behavior_Cloning_Drive.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "scrolled": true,
        "id": "fCDPD9Cg2kYl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Author: Suhan Shetty | suhan.n.shetty@gmail.com\n",
        "# This is an implementation of behavior cloning for different agents available in openai gym environrment. \n",
        "# The expert policy is already available and the dataset-[observations, actions] is included as expert_data\n",
        "#  for different environments.\n",
        "\n",
        "# Jupyter-notebook shortcuts (Press Esc first):\n",
        "# Cmd + Shift + P - pops up keyboard shortcuts\n",
        "# Shift+L - toggles line numbering\n",
        "# Ctrl+Enter - Run the current Cell\n",
        "# Shift+Tab - indent / de-indent\n",
        "# D + D - delete the current cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kizKlxg-6vyB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# # Execute this section to Setup your Google Drive with Colab. This will help in saving the model directly to your Google Drive\n",
        "# # Reference: https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q#scrollTo=c99EvWo1s9-x\n",
        "\n",
        "# # Install a Drive FUSE wrapper.\n",
        "# # https://github.com/astrada/google-drive-ocamlfuse\n",
        "# !apt-get update -qq 2>&1 > /dev/null\n",
        "# !apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "# !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "# !apt-get update -qq 2>&1 > /dev/null\n",
        "# !apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "# # Generate auth tokens for Colab\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "\n",
        "# # Generate creds for the Drive FUSE library.\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "# creds = GoogleCredentials.get_application_default()\n",
        "# import getpass\n",
        "# # Work around misordering of STREAM and STDIN in Jupyter.\n",
        "# # https://github.com/jupyter/notebook/issues/3159\n",
        "# prompt = !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "# vcode = getpass.getpass(prompt[0] + '\\n\\nEnter verification code: ')\n",
        "# !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "# # Create a directory and mount Google Drive using that directory.\n",
        "# !mkdir -p drive\n",
        "# !google-drive-ocamlfuse drive\n",
        "\n",
        "# print('Files in Drive:')\n",
        "# !ls drive/\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l01E9tHv7T83",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "4a1433fb-d9dc-4131-c63b-6df7c2ca5ac1"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "!ls drive/drive_bc/expert_data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ant-v2.pkl\t    Hopper-v2.pkl    Reacher-v2.pkl\n",
            "HalfCheetah-v2.pkl  Humanoid-v2.pkl  Walker2d-v2.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "y00vMYXj2kYo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "63d1cc90-d48f-47f8-8413-80daafbaad34"
      },
      "cell_type": "code",
      "source": [
        "# Walker2d-v2\n",
        "# Import the data from an expert policy: [observations, actions]\n",
        "import numpy as np\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "\n",
        "# read the dataset: expert policy\n",
        "agent = \"./drive/drive_bc/expert_data/Humanoid-v2.pkl\"\n",
        "expert_policy = pickle.load( open( agent, \"rb\" ) )\n",
        "\n",
        "# separate datset into input and output\n",
        "observations_data = expert_policy['observations'] \n",
        "actions_data = expert_policy['actions'] \n",
        "print(\"-------------------------------------------------------\")\n",
        "print(\"Shape of Input: \", observations_data.shape)\n",
        "print(\"Shape of Output: \", actions_data.shape)\n",
        "\n",
        "# squeeze the ouput_data matrix to 2D array\n",
        "actions_data = np.squeeze(actions_data, axis=1)\n",
        "print(\"Reshaped Output: \",actions_data.shape)\n",
        "\n",
        "# verify the shape of the data \n",
        "assert observations_data.shape[0] == actions_data.shape[0] \n",
        "assert (actions_data.ndim == 2)&(actions_data.ndim == 2)\n",
        "print(\"-------------------------------------------------------\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------\n",
            "Shape of Input:  (19152, 376)\n",
            "Shape of Output:  (19152, 1, 17)\n",
            "Reshaped Output:  (19152, 17)\n",
            "-------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hB1XwdCV2kYs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "242d12fd-34f7-48ac-949d-9817d3373e67"
      },
      "cell_type": "code",
      "source": [
        "# Separate dataset into training and test data\n",
        "data_size = observations_data.shape[0]\n",
        "test_size = int(data_size/8)\n",
        "\n",
        "index = np.random.choice(range(data_size), size=test_size, replace=False)\n",
        "test_observations_data = observations_data[index,:]\n",
        "test_actions_data = actions_data[index,:]\n",
        "\n",
        "# exclude the test data from training data\n",
        "observations_data = np.delete(observations_data, index, axis=0)\n",
        "actions_data = np.delete(actions_data, index, axis=0)\n",
        "\n",
        "print(\"-------------------------------------------------------\")\n",
        "print(\"Size of input data, training: \", observations_data.shape)\n",
        "print(\"Size of output data, training: \", actions_data.shape)\n",
        "print(\"Size of input data, testing: \", test_observations_data.shape)\n",
        "print(\"Size of output data, testing: \", test_actions_data.shape)\n",
        "print(\"-------------------------------------------------------\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------\n",
            "Size of input data, training:  (16758, 376)\n",
            "Size of output data, training:  (16758, 17)\n",
            "Size of input data, testing:  (2394, 376)\n",
            "Size of output data, testing:  (2394, 17)\n",
            "-------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "V9tLBQdj2kYv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "35531a60-20a8-44a5-9a4f-aafa0cee24e6"
      },
      "cell_type": "code",
      "source": [
        "# Setup tensorflow\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "print(\"tf version: \", tf.VERSION)\n",
        "print(\"GPU: \", tf.test.gpu_device_name())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf version:  1.12.0-rc2\n",
            "GPU:  /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N_sYwlWC2kYz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Start a tf session\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "O7hVO8T_2kY3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Tip: if you run into problems with TensorBoard\n",
        "# clear the contents of this directory, re-run this script\n",
        "# then restart TensorBoard to see the result\n",
        "LOGDIR = './graphs'\n",
        "\n",
        "if not os.path.exists(LOGDIR):\n",
        "    os.makedirs(LOGDIR)\n",
        "\n",
        "\n",
        "### Tensor Board Setup\n",
        "writer = tf.summary.FileWriter(LOGDIR)\n",
        "writer.add_graph(tf.get_default_graph())\n",
        "\n",
        "#!tensorboard --logdir ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hafBkj5Y2kY5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DNN Architeture : \n",
        "# Ref: https://github.com/tensorflow/workshops/blob/master/extras/archive/03_deep_neural_network_low_level.ipynb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WuXK_KxL2kZE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Hyper-parameters\n",
        "LAYER_SIZE=64\n",
        "LEARNING_RATE = 0.0005\n",
        "TRAIN_STEPS = 50000\n",
        "BATCH_SIZE = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ITYrzZAz2kZI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "efb3c267-c742-4fed-b9fd-24c47a4d8305"
      },
      "cell_type": "code",
      "source": [
        "# Define inputs\n",
        "with tf.name_scope('input'):\n",
        "    observations = tf.placeholder(tf.float32, [None, observations_data.shape[1]], name=\"observations\")\n",
        "    actions = tf.placeholder(tf.float32, [None, actions_data.shape[1]], name=\"actions\")\n",
        "\n",
        "print(\"observations tensor: \", observations)\n",
        "print(\"actions tensor: \", actions)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "observations tensor:  Tensor(\"input/observations:0\", shape=(?, 376), dtype=float32)\n",
            "actions tensor:  Tensor(\"input/actions:0\", shape=(?, 17), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rXylCBDG2kZN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Ref:https://www.tensorflow.org/api_docs/python/tf/layers/dense\n",
        "\n",
        "\n",
        "with tf.name_scope('layers'):\n",
        "    fc1 = tf.layers.dense(inputs=observations,  activation=tf.nn.relu, units=LAYER_SIZE, name='fc1')\n",
        "    dropped_1 = tf.nn.dropout(fc1, keep_prob=0.9)\n",
        "    fc2 = tf.layers.dense(inputs=dropped_1,  activation=tf.nn.relu, units=LAYER_SIZE, name='fc2')\n",
        "    dropped_2 = tf.nn.dropout(fc2, keep_prob=0.9)\n",
        "    fc_last = tf.layers.dense(inputs=dropped_2,  activation=tf.nn.relu, units=LAYER_SIZE, name='fc_last_hidden')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QCUTsA8v2kZQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# output layer\n",
        "with tf.name_scope('output'):\n",
        "    final_output = tf.layers.dense(inputs=fc_last,  activation=None, units=actions_data.shape[1], name='final_output')\n",
        "    actions_pred = tf.identity(final_output, name='actions_pred')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d-XdX5_72kZV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define loss and an optimizer\n",
        "with tf.name_scope(\"loss\"):\n",
        "    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=labels))\n",
        "    loss = tf.reduce_mean(tf.square(actions_pred - actions))\n",
        "    tf.summary.scalar('loss', loss)\n",
        "\n",
        "with tf.name_scope(\"optimizer\"):\n",
        "    train = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CPHDHwhW2kZb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set up logging.\n",
        "# We'll use a second FileWriter to summarize accuracy on\n",
        "# the test set. This will let us display it nicely in TensorBoard.\n",
        "train_writer = tf.summary.FileWriter(os.path.join(LOGDIR, \"train\"))\n",
        "train_writer.add_graph(sess.graph)\n",
        "test_writer = tf.summary.FileWriter(os.path.join(LOGDIR, \"test\"))\n",
        "summary_op = tf.summary.merge_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b4uwt4Tg2kZd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0DFzf1jX2kZg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to sample a batch of data from training set\n",
        "def sample_data(observations_data, actions_data, batch_size):\n",
        "    index = np.random.choice(range(observations_data.shape[0]),size=batch_size, replace=False)\n",
        "    sample_actions = actions_data[index,:]\n",
        "    sample_observations = observations_data[index,:]\n",
        "    \n",
        "    return sample_observations, sample_actions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L3MkJkyw2kZh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 913
        },
        "outputId": "0759d969-041b-401b-918f-f20552df4b78"
      },
      "cell_type": "code",
      "source": [
        "# training\n",
        "\n",
        "#Create a saver object which will save all the variables\n",
        "\n",
        "export_dir = \"./drive/drive_bc/saved_model_humanoid/\" \n",
        "if not os.path.exists(export_dir):\n",
        "    os.makedirs(export_dir)\n",
        "    \n",
        "saver = tf.train.Saver()\n",
        "\n",
        "for step in range(TRAIN_STEPS):\n",
        "    batch_obs, batch_actions = sample_data(observations_data, actions_data, BATCH_SIZE )\n",
        "    #batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE)\n",
        "    summary_result, mse_run, _ = sess.run([summary_op, loss, train], \n",
        "                                    feed_dict={observations: batch_obs, actions: batch_actions})\n",
        "\n",
        "    train_writer.add_summary(summary_result, step)\n",
        "    train_writer.add_run_metadata(tf.RunMetadata(), 'step%03d' % step)\n",
        "    \n",
        "    # calculate accuracy on the test set, every 100 steps.\n",
        "    # we're using the entire test set here, so this will be a bit slow\n",
        "    if step % 1000 == 0:\n",
        "        test_writer.add_summary(summary_result, step)\n",
        "        print('train_step: {0:04d} mse: {1:.3f}'.format(step, mse_run))\n",
        "        #save the graph\n",
        "        saver.save(sess, export_dir+\"/\"+\"agent_name\")\n",
        "\n",
        "train_writer.close()\n",
        "test_writer.close()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_step: 0000 mse: 919.449\n",
            "train_step: 1000 mse: 1.074\n",
            "train_step: 2000 mse: 0.788\n",
            "train_step: 3000 mse: 0.662\n",
            "train_step: 4000 mse: 0.563\n",
            "train_step: 5000 mse: 0.468\n",
            "train_step: 6000 mse: 0.397\n",
            "train_step: 7000 mse: 0.374\n",
            "train_step: 8000 mse: 0.386\n",
            "train_step: 9000 mse: 0.350\n",
            "train_step: 10000 mse: 0.341\n",
            "train_step: 11000 mse: 0.331\n",
            "train_step: 12000 mse: 0.343\n",
            "train_step: 13000 mse: 0.311\n",
            "train_step: 14000 mse: 0.235\n",
            "train_step: 15000 mse: 0.211\n",
            "train_step: 16000 mse: 0.179\n",
            "train_step: 17000 mse: 0.167\n",
            "train_step: 18000 mse: 0.142\n",
            "train_step: 19000 mse: 0.145\n",
            "train_step: 20000 mse: 0.138\n",
            "train_step: 21000 mse: 0.119\n",
            "train_step: 22000 mse: 0.127\n",
            "train_step: 23000 mse: 0.106\n",
            "train_step: 24000 mse: 0.097\n",
            "train_step: 25000 mse: 0.100\n",
            "train_step: 26000 mse: 0.088\n",
            "train_step: 27000 mse: 0.086\n",
            "train_step: 28000 mse: 0.087\n",
            "train_step: 29000 mse: 0.080\n",
            "train_step: 30000 mse: 0.097\n",
            "train_step: 31000 mse: 0.082\n",
            "train_step: 32000 mse: 0.080\n",
            "train_step: 33000 mse: 0.081\n",
            "train_step: 34000 mse: 0.073\n",
            "train_step: 35000 mse: 0.082\n",
            "train_step: 36000 mse: 0.072\n",
            "train_step: 37000 mse: 0.077\n",
            "train_step: 38000 mse: 0.069\n",
            "train_step: 39000 mse: 0.068\n",
            "train_step: 40000 mse: 0.068\n",
            "train_step: 41000 mse: 0.070\n",
            "train_step: 42000 mse: 0.075\n",
            "train_step: 43000 mse: 0.070\n",
            "train_step: 44000 mse: 0.063\n",
            "train_step: 45000 mse: 0.071\n",
            "train_step: 46000 mse: 0.069\n",
            "train_step: 47000 mse: 0.065\n",
            "train_step: 48000 mse: 0.068\n",
            "train_step: 49000 mse: 0.066\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}