{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Author: Suhan Shetty | suhan.n.shetty@gmail.com\n",
    "# This is an implementation of behavior cloning for different agents available in openai gym environrment. \n",
    "# The expert policy is already available and the dataset-[observations, actions] is included as expert_data\n",
    "#  for different environments.\n",
    "\n",
    "#Ref:http://rail.eecs.berkeley.edu/deeprlcourse/\n",
    "\n",
    "# Jupyter-notebook shortcuts (Press Esc first):\n",
    "# Cmd + Shift + P - pops up keyboard shortcuts\n",
    "# Shift+L - toggles line numbering\n",
    "# Ctrl+Enter - Run the current Cell\n",
    "# Shift+Tab - indent / de-indent\n",
    "# D + D - delete the current cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Shape of Input:  (19152, 376)\n",
      "Shape of Output:  (19152, 1, 17)\n",
      "Reshaped Output:  (19152, 17)\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Import the data from an expert policy: [observations, actions]\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "# read the dataset\n",
    "agent = \"expert_data/Humanoid-v2.pkl\"\n",
    "expert_policy = pickle.load( open( agent, \"rb\" ) )\n",
    "\n",
    "# separate datset into input and output\n",
    "observations_data = expert_policy['observations'] \n",
    "actions_data = expert_policy['actions'] \n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"Shape of Input: \", observations_data.shape)\n",
    "print(\"Shape of Output: \", actions_data.shape)\n",
    "\n",
    "# squeeze the ouput_data matrix to 2D array\n",
    "actions_data = np.squeeze(actions_data, axis=1)\n",
    "print(\"Reshaped Output: \",actions_data.shape)\n",
    "\n",
    "# verify the shape of the data \n",
    "assert observations_data.shape[0] == actions_data.shape[0] \n",
    "assert (actions_data.ndim == 2)&(actions_data.ndim == 2)\n",
    "print(\"-------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Size of input data, training:  (17237, 376)\n",
      "Size of output data, training:  (17237, 17)\n",
      "Size of input data, testing:  (1915, 376)\n",
      "Size of output data, testing:  (1915, 17)\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Separate dataset into training and test data\n",
    "data_size = observations_data.shape[0]\n",
    "test_size = int(data_size/10)\n",
    "\n",
    "index = np.random.choice(range(data_size), size=test_size, replace=False)\n",
    "test_observations_data = observations_data[index,:]\n",
    "test_actions_data = actions_data[index,:]\n",
    "\n",
    "# exclude the test data from training data\n",
    "observations_data = np.delete(observations_data, index, axis=0)\n",
    "actions_data = np.delete(actions_data, index, axis=0)\n",
    "\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"Size of input data, training: \", observations_data.shape)\n",
    "print(\"Size of output data, training: \", actions_data.shape)\n",
    "print(\"Size of input data, testing: \", test_observations_data.shape)\n",
    "print(\"Size of output data, testing: \", test_actions_data.shape)\n",
    "print(\"-------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version:  1.11.0\n"
     ]
    }
   ],
   "source": [
    "# Setup tensorflow\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"tf version: \", tf.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a tf session\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tip: if you run into problems with TensorBoard\n",
    "# clear the contents of this directory, re-run this script\n",
    "# then restart TensorBoard to see the result\n",
    "LOGDIR = './graphs'\n",
    "\n",
    "if not os.path.exists(LOGDIR):\n",
    "    os.makedirs(LOGDIR)\n",
    "\n",
    "\n",
    "### Tensor Board Setup\n",
    "writer = tf.summary.FileWriter(LOGDIR)\n",
    "writer.add_graph(tf.get_default_graph())\n",
    "\n",
    "#!tensorboard --logdir ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN Architeture : \n",
    "# Ref: https://github.com/tensorflow/workshops/blob/master/extras/archive/03_deep_neural_network_low_level.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "LAYER_SIZE=64\n",
    "LEARNING_RATE = 0.001\n",
    "TRAIN_STEPS = 100000\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observations tensor:  Tensor(\"input/observations:0\", shape=(?, 376), dtype=float32)\n",
      "actions tensor:  Tensor(\"input/actions:0\", shape=(?, 17), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Define inputs\n",
    "with tf.name_scope('input'):\n",
    "    observations = tf.placeholder(tf.float32, [None, observations_data.shape[1]], name=\"observations\")\n",
    "    actions = tf.placeholder(tf.float32, [None, actions_data.shape[1]], name=\"actions\")\n",
    "\n",
    "print(\"observations tensor: \", observations)\n",
    "print(\"actions tensor: \", actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref:https://www.tensorflow.org/api_docs/python/tf/layers/dense\n",
    "\n",
    "\n",
    "with tf.name_scope('layers'):\n",
    "    fc1 = tf.layers.dense(inputs=observations,  activation=tf.nn.relu, units=LAYER_SIZE, name='fc1')\n",
    "    dropped_1 = tf.nn.dropout(fc1, keep_prob=0.9)\n",
    "    fc2 = tf.layers.dense(inputs=dropped_1,  activation=tf.nn.relu, units=LAYER_SIZE, name='fc2')\n",
    "    dropped_2 = tf.nn.dropout(fc2, keep_prob=0.9)\n",
    "    fc_last = tf.layers.dense(inputs=dropped_2,  activation=tf.nn.relu, units=LAYER_SIZE, name='fc_last_hidden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output layer\n",
    "with tf.name_scope('output'):\n",
    "    final_output = tf.layers.dense(inputs=fc_last,  activation=None, units=actions_data.shape[1], name='final_output')\n",
    "    actions_pred = tf.identity(final_output, name='actions_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and an optimizer\n",
    "with tf.name_scope(\"loss\"):\n",
    "    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=labels))\n",
    "    loss = tf.reduce_mean(tf.square(actions_pred - actions))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    train = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging.\n",
    "# We'll use a second FileWriter to summarize accuracy on\n",
    "# the test set. This will let us display it nicely in TensorBoard.\n",
    "train_writer = tf.summary.FileWriter(os.path.join(LOGDIR, \"train\"))\n",
    "train_writer.add_graph(sess.graph)\n",
    "test_writer = tf.summary.FileWriter(os.path.join(LOGDIR, \"test\"))\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample a batch of data from training set\n",
    "def sample_data(observations_data, actions_data, batch_size):\n",
    "    index = np.random.choice(range(observations_data.shape[0]),size=batch_size, replace=False)\n",
    "    sample_actions = actions_data[index,:]\n",
    "    sample_observations = observations_data[index,:]\n",
    "    \n",
    "    return sample_observations, sample_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_step: 0000 mse: 1913.418\n",
      "train_step: 1000 mse: 0.532\n",
      "train_step: 2000 mse: 0.447\n",
      "train_step: 3000 mse: 0.386\n",
      "train_step: 4000 mse: 0.332\n",
      "train_step: 5000 mse: 0.317\n",
      "train_step: 6000 mse: 0.269\n",
      "train_step: 7000 mse: 0.258\n",
      "train_step: 8000 mse: 0.212\n",
      "train_step: 9000 mse: 0.169\n",
      "train_step: 10000 mse: 0.151\n",
      "train_step: 11000 mse: 0.150\n",
      "train_step: 12000 mse: 0.119\n",
      "train_step: 13000 mse: 0.118\n",
      "train_step: 14000 mse: 0.115\n",
      "train_step: 15000 mse: 0.107\n",
      "train_step: 16000 mse: 0.094\n",
      "train_step: 17000 mse: 0.100\n",
      "train_step: 18000 mse: 0.094\n",
      "train_step: 19000 mse: 0.089\n",
      "train_step: 20000 mse: 0.094\n",
      "train_step: 21000 mse: 0.089\n",
      "train_step: 22000 mse: 0.087\n",
      "train_step: 23000 mse: 0.087\n",
      "train_step: 24000 mse: 0.085\n",
      "train_step: 25000 mse: 0.079\n",
      "train_step: 26000 mse: 0.081\n",
      "train_step: 27000 mse: 0.074\n",
      "train_step: 28000 mse: 0.074\n",
      "train_step: 29000 mse: 0.075\n",
      "train_step: 30000 mse: 0.076\n",
      "train_step: 31000 mse: 0.071\n",
      "train_step: 32000 mse: 0.085\n",
      "train_step: 33000 mse: 0.066\n",
      "train_step: 34000 mse: 0.063\n",
      "train_step: 35000 mse: 0.068\n",
      "train_step: 36000 mse: 0.070\n",
      "train_step: 37000 mse: 0.073\n",
      "train_step: 38000 mse: 0.068\n",
      "train_step: 39000 mse: 0.069\n",
      "train_step: 40000 mse: 0.063\n",
      "train_step: 41000 mse: 0.070\n",
      "train_step: 42000 mse: 0.067\n",
      "train_step: 43000 mse: 0.062\n",
      "train_step: 44000 mse: 0.066\n",
      "train_step: 45000 mse: 0.066\n",
      "train_step: 46000 mse: 0.063\n",
      "train_step: 47000 mse: 0.070\n",
      "train_step: 48000 mse: 0.066\n",
      "train_step: 49000 mse: 0.062\n",
      "train_step: 50000 mse: 0.068\n",
      "train_step: 51000 mse: 0.068\n",
      "train_step: 52000 mse: 0.063\n",
      "train_step: 53000 mse: 0.061\n",
      "train_step: 54000 mse: 0.056\n",
      "train_step: 55000 mse: 0.062\n",
      "train_step: 56000 mse: 0.064\n",
      "train_step: 57000 mse: 0.062\n",
      "train_step: 58000 mse: 0.059\n",
      "train_step: 59000 mse: 0.060\n",
      "train_step: 60000 mse: 0.064\n",
      "train_step: 61000 mse: 0.057\n",
      "train_step: 62000 mse: 0.063\n",
      "train_step: 63000 mse: 0.062\n",
      "train_step: 64000 mse: 0.061\n",
      "train_step: 65000 mse: 0.066\n",
      "train_step: 66000 mse: 0.060\n",
      "train_step: 67000 mse: 0.063\n",
      "train_step: 68000 mse: 0.063\n",
      "train_step: 69000 mse: 0.063\n",
      "train_step: 70000 mse: 0.061\n",
      "train_step: 71000 mse: 0.059\n",
      "train_step: 72000 mse: 0.057\n",
      "train_step: 73000 mse: 0.057\n",
      "train_step: 74000 mse: 0.060\n",
      "train_step: 75000 mse: 0.060\n",
      "train_step: 76000 mse: 0.065\n",
      "train_step: 77000 mse: 0.062\n",
      "train_step: 78000 mse: 0.063\n",
      "train_step: 79000 mse: 0.059\n",
      "train_step: 80000 mse: 0.057\n",
      "train_step: 81000 mse: 0.060\n",
      "train_step: 82000 mse: 0.058\n",
      "train_step: 83000 mse: 0.059\n",
      "train_step: 84000 mse: 0.059\n",
      "train_step: 85000 mse: 0.062\n",
      "train_step: 86000 mse: 0.057\n",
      "train_step: 87000 mse: 0.058\n",
      "train_step: 88000 mse: 0.056\n",
      "train_step: 89000 mse: 0.055\n",
      "train_step: 90000 mse: 0.058\n",
      "train_step: 91000 mse: 0.056\n",
      "train_step: 92000 mse: 0.055\n",
      "train_step: 93000 mse: 0.068\n",
      "train_step: 94000 mse: 0.070\n",
      "train_step: 95000 mse: 0.058\n",
      "train_step: 96000 mse: 0.064\n",
      "train_step: 97000 mse: 0.058\n",
      "train_step: 98000 mse: 0.058\n",
      "train_step: 99000 mse: 0.058\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "#Create a saver object which will save all the variables\n",
    "saver = tf.train.Saver()\n",
    "agent_name = \"humanoid_2\"\n",
    "export_path = \"./saved_model_\"+agent_name\n",
    "if not os.path.exists(export_path):\n",
    "    os.makedirs(export_path)\n",
    "    \n",
    "    \n",
    "\n",
    "for step in range(TRAIN_STEPS):\n",
    "    batch_obs, batch_actions = sample_data(observations_data, actions_data, BATCH_SIZE )\n",
    "    #batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "    summary_result, mse_run, _ = sess.run([summary_op, loss, train], \n",
    "                                    feed_dict={observations: batch_obs, actions: batch_actions})\n",
    "\n",
    "    train_writer.add_summary(summary_result, step)\n",
    "    train_writer.add_run_metadata(tf.RunMetadata(), 'step%03d' % step)\n",
    "    \n",
    "    # calculate accuracy on the test set, every 100 steps.\n",
    "    # we're using the entire test set here, so this will be a bit slow\n",
    "    if step % 1000 == 0:\n",
    "        test_writer.add_summary(summary_result, step)\n",
    "        print('train_step: {0:04d} mse: {1:.3f}'.format(step, mse_run))\n",
    "        #save the graph\n",
    "        saver.save(sess, export_path+\"/\"+agent_name)\n",
    "        \n",
    "train_writer.close()\n",
    "test_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
